# Alertmanager Configuration Values
# Environment-specific configurations for notification channels and routing

# Global configuration
global:
  # AWS account ID
  awsAccountId: ""
  # AWS region
  region: "us-west-2"
  # Environment name (dev, staging, prod)
  environment: ""
  # Project name
  projectName: "cybersentinel"
  # Domain for external access
  domain: "cybersentinel.company.com"

# Alertmanager configuration
alertmanager:
  # Image configuration
  image:
    repository: prom/alertmanager
    tag: v0.26.0
    pullPolicy: IfNotPresent

  # Deployment configuration
  replicas: 3  # HA configuration
  
  # Resource configuration
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  # Storage configuration
  persistence:
    enabled: true
    storageClass: gp3
    size: 10Gi
    retention: "120h"  # 5 days
  
  # Security configuration
  securityContext:
    runAsNonRoot: true
    runAsUser: 65534
    runAsGroup: 65534
    fsGroup: 65534
  
  # Network configuration
  service:
    type: ClusterIP
    port: 9093
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9093"
      prometheus.io/path: "/metrics"
  
  # External URL configuration
  externalUrl: "https://alertmanager.{{ .Values.global.domain }}"
  
  # Clustering configuration for HA
  cluster:
    enabled: true
    port: 9094
    reconnectTimeout: "5m"

# Notification channels configuration
notifications:
  # Slack configuration
  slack:
    enabled: true
    # Webhook URL will be retrieved from External Secrets
    webhookUrlSecret: "alertmanager-secrets"
    webhookUrlKey: "slack_webhook_url"
    
    # Channel configuration by environment and severity
    channels:
      critical:
        prod: "#alerts-critical"
        staging: "#alerts-staging"
        dev: "#dev-alerts"
      warning:
        prod: "#alerts"
        staging: "#alerts"
        dev: "#dev-alerts"
      infrastructure: "#infrastructure"
      database: "#database"
      application: "#application"
    
    # Message formatting
    titleLink: "https://grafana.{{ .Values.global.domain }}"
    iconEmoji: ":warning:"
  
  # PagerDuty configuration
  pagerduty:
    enabled: true
    # API URL
    url: "https://events.pagerduty.com/v2/enqueue"
    # Routing key will be retrieved from External Secrets
    routingKeySecret: "alertmanager-secrets"
    routingKeyKey: "pagerduty_routing_key"
    
    # Integration configuration
    severity: "{{ .CommonLabels.severity }}"
    description: "CyberSentinel {{ .CommonLabels.environment }} Alert: {{ .GroupLabels.alertname }}"
    
    # Links
    links:
    - href: "https://grafana.{{ .Values.global.domain }}"
      text: "Grafana Dashboard"
    - href: "https://prometheus.{{ .Values.global.domain }}"
      text: "Prometheus"
  
  # Email configuration
  email:
    enabled: true
    # SMTP configuration will be retrieved from External Secrets
    smtpHost: "smtp.company.com"
    smtpPort: 587
    smtpFrom: "cybersentinel-alerts@company.com"
    
    # SMTP credentials from External Secrets
    smtpUsernameSecret: "alertmanager-secrets"
    smtpUsernameKey: "smtp_username"
    smtpPasswordSecret: "alertmanager-secrets"
    smtpPasswordKey: "smtp_password"
    
    # Team distribution lists
    teams:
      infrastructure: "infrastructure-team@company.com"
      database: "database-team@company.com"
      application: "application-team@company.com"
      security: "security-team@company.com"
  
  # Webhook configuration for custom integrations
  webhook:
    enabled: true
    endpoints:
      default: "http://localhost:5001/webhook"
      deadmansswitch: "https://deadmanssnitch.com/{{ .Values.deadMansSwitchToken }}"

# Alert routing configuration
routing:
  # Global routing settings
  groupBy: ['alertname', 'environment', 'severity']
  groupWait: "30s"
  groupInterval: "5m"
  repeatInterval: "12h"
  
  # Environment-specific routing
  environments:
    prod:
      critical:
        groupWait: "10s"
        groupInterval: "2m"
        repeatInterval: "5m"
        receivers: ['critical-pagerduty-slack']
      warning:
        groupWait: "2m"
        groupInterval: "10m"
        repeatInterval: "4h"
        receivers: ['warning-slack']
    
    staging:
      critical:
        groupWait: "10s"
        groupInterval: "2m"
        repeatInterval: "5m"
        receivers: ['critical-slack']
      warning:
        groupWait: "2m"
        groupInterval: "10m"
        repeatInterval: "4h"
        receivers: ['warning-slack']
    
    dev:
      critical:
        groupWait: "30s"
        groupInterval: "5m"
        repeatInterval: "30m"
        receivers: ['dev-slack']
      warning:
        groupWait: "5m"
        groupInterval: "15m"
        repeatInterval: "2h"
        receivers: ['dev-slack']

# Alert inhibition rules
inhibition:
  enabled: true
  rules:
  # Inhibit warning alerts if critical alert is firing for same service
  - sourceMatch:
      severity: 'critical'
    targetMatch:
      severity: 'warning'
    equal: ['alertname', 'environment', 'instance']
  
  # Inhibit API-specific alerts if API is completely down
  - sourceMatch:
      alertname: 'CyberSentinelAPIDown'
    targetMatchRe:
      alertname: '^CyberSentinel(HighResponseTime|HighErrorRate)$'
    equal: ['instance']
  
  # Inhibit application alerts if node is down
  - sourceMatch:
      alertname: 'KubernetesNodeNotReady'
    targetMatchRe:
      alertname: '^CyberSentinel.*'
    equal: ['node']
  
  # Inhibit database connection alerts if database is down
  - sourceMatchRe:
      alertname: '^.*Database.*Down$'
    targetMatchRe:
      alertname: '^.*Database.*Connection.*'
    equal: ['database_name']

# External Secrets integration
externalSecrets:
  enabled: true
  secretStore: "cybersentinel-aws-secrets"
  
  # Secrets to retrieve from AWS Secrets Manager
  secrets:
  - name: "alertmanager-secrets"
    secretKeys:
    - secretKey: "slack_webhook_url"
      remoteRef:
        key: "{{ .Values.global.projectName }}-{{ .Values.global.environment }}-external-services"
        property: "slack_webhook_url"
    
    - secretKey: "pagerduty_routing_key"
      remoteRef:
        key: "{{ .Values.global.projectName }}-{{ .Values.global.environment }}-external-services"
        property: "pagerduty_api_key"
    
    - secretKey: "smtp_username"
      remoteRef:
        key: "{{ .Values.global.projectName }}-{{ .Values.global.environment }}-external-services"
        property: "smtp_username"
    
    - secretKey: "smtp_password"
      remoteRef:
        key: "{{ .Values.global.projectName }}-{{ .Values.global.environment }}-external-services"
        property: "smtp_password"

# Dead man's switch configuration
deadMansSwitch:
  enabled: true
  # Token for dead man's snitch service
  token: ""  # Will be set per environment
  interval: "1m"
  timeout: "5m"

# Monitoring configuration
monitoring:
  serviceMonitor:
    enabled: true
    interval: "30s"
    scrapeTimeout: "10s"
    path: "/metrics"
  
  # Alertmanager self-monitoring alerts
  selfMonitoring:
    enabled: true
    alerts:
    - alertName: "AlertmanagerDown"
      expression: "up{job='alertmanager'} == 0"
      duration: "5m"
      severity: "critical"
      summary: "Alertmanager instance is down"
    
    - alertName: "AlertmanagerConfigurationReload"
      expression: "alertmanager_config_last_reload_successful == 0"
      duration: "5m"
      severity: "warning"
      summary: "Alertmanager configuration reload failed"
    
    - alertName: "AlertmanagerNotificationsFailing"
      expression: "rate(alertmanager_notifications_failed_total[5m]) > 0"
      duration: "5m"
      severity: "warning"
      summary: "Alertmanager is failing to send notifications"

# Security configuration
security:
  networkPolicy:
    enabled: true
    # Allow ingress from Prometheus and Grafana
    ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
          podSelector:
            matchLabels:
              app: prometheus
      ports:
      - protocol: TCP
        port: 9093
    
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
          podSelector:
            matchLabels:
              app.kubernetes.io/name: grafana
      ports:
      - protocol: TCP
        port: 9093
    
    # Allow cluster communication
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
          podSelector:
            matchLabels:
              app.kubernetes.io/name: alertmanager
      ports:
      - protocol: TCP
        port: 9094
    
    # Egress rules
    egress:
    # DNS resolution
    - to: []
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    
    # HTTPS for webhooks
    - to: []
      ports:
      - protocol: TCP
        port: 443
    
    # SMTP for email
    - to: []
      ports:
      - protocol: TCP
        port: 587
      - protocol: TCP
        port: 25
    
    # Cluster communication
    - to:
        - namespaceSelector:
            matchLabels:
              name: monitoring
          podSelector:
            matchLabels:
              app.kubernetes.io/name: alertmanager
      ports:
      - protocol: TCP
        port: 9094

# Environment-specific configurations
environments:
  dev:
    alertmanager:
      replicas: 1  # Single instance for dev
      persistence:
        size: 5Gi
    
    notifications:
      slack:
        channels:
          critical: "#dev-alerts"
          warning: "#dev-alerts"
      pagerduty:
        enabled: false  # No PagerDuty for dev
      email:
        enabled: false  # No email for dev
    
    deadMansSwitch:
      enabled: false  # No dead man's switch for dev
  
  staging:
    alertmanager:
      replicas: 2  # Reduced HA for staging
      persistence:
        size: 8Gi
    
    notifications:
      slack:
        channels:
          critical: "#alerts-staging"
          warning: "#alerts-staging"
      pagerduty:
        enabled: false  # No PagerDuty for staging
      email:
        enabled: true
        teams:
          infrastructure: "staging-alerts@company.com"
    
    deadMansSwitch:
      enabled: true
      token: "staging-deadmansswitch-token"
  
  prod:
    alertmanager:
      replicas: 3  # Full HA for production
      persistence:
        size: 10Gi
        retention: "168h"  # 7 days for production
    
    notifications:
      slack:
        channels:
          critical: "#alerts-critical"
          warning: "#alerts"
      pagerduty:
        enabled: true
      email:
        enabled: true
    
    deadMansSwitch:
      enabled: true
      token: "prod-deadmansswitch-token"

# Additional labels and annotations
extraLabels:
  app.kubernetes.io/component: "alerting"
  app.kubernetes.io/part-of: "cybersentinel"
  app.kubernetes.io/managed-by: "helm"

extraAnnotations:
  alerting.cybersentinel.io/version: "v1.0"
  alerting.cybersentinel.io/config-version: ""